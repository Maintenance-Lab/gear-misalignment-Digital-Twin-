{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKC-y-we9Z4s"
      },
      "source": [
        " # imports\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhhEG48w0zNN"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "stable = True # Set to True for latest pip version or False for main branch in GitHub\n",
        "!pip install {\"tsai -U\" if stable else \"git+https://github.com/timeseriesAI/tsai.git\"} >> /dev/null\n",
        "from tsai.all import *\n",
        "computer_setup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fE0H6ngH1LOC"
      },
      "outputs": [],
      "source": [
        "!pip install wandb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import torch         \n",
        "import wandb \n",
        "from fastai.callback.wandb import *\n",
        "import pandas as pd\n",
        "my_setup(wandb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArJbFqvQW0UV"
      },
      "outputs": [],
      "source": [
        "# use when using google drive \n",
        "# data is csv \n",
        "#data = np.genfromtxt('/content/drive/MyDrive/digitalTwin_data/data_dump_train_groot_dag2V2.csv', delimiter=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1cQ2xuSiwdD"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/Users/emile/Downloads/data_dump_train_groot_dag2V2.csv')\n",
        "data = df.to_numpy()\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SKN3XW5W1jA"
      },
      "outputs": [],
      "source": [
        "# min rij 1 en col 1 (index and headers) \n",
        "# data = data[1:,:]\n",
        "# data = data[:,1:]\n",
        "\n",
        "name = ['time','pos_estimate_main','pos_estimate_rotate','epower','ppower','vel_estimate','pos_setpoint','gyroy','gyrox','gyroz']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQoA7NbogF6g"
      },
      "outputs": [],
      "source": [
        "# t to dt \n",
        "for i in range(len(data)):\n",
        "    data[i,0] = data[i,0] - data[i+1,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpAPth9nW1oT"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(learner):\n",
        "  dls = learn2.dls\n",
        "  valid_dl = dls.valid\n",
        "  valid_probas, valid_targets, valid_preds = learn2.get_preds(dl=valid_dl, with_decoded=True)\n",
        "  loss = torch.mean(abs(g))\n",
        "  plt.plot(valid_targets[:])\n",
        "  plt.plot(valid_preds[:])\n",
        "  plt.show()\n",
        "  fig.savefig('model_{},{},{},{}'.format(coloms, window, epochs, lr), bbox_inches='tight', dpi=150)\n",
        "  \n",
        "  return loss\n",
        "\n",
        "\n",
        "def train_get_metrics(coloms, window, epochs, lr,data_len,hz):\n",
        "  print(\"Run{}_{}_{}_{}_{}_{}\".format(coloms, window, epochs, lr,hz,data_len,))\n",
        "  df = pd.read_csv('/Users/emile/Downloads/data_dump_train_groot_dag2V2.csv')\n",
        "  data = df.to_numpy()\n",
        "  del df\n",
        "  data = data[:data_len,:]\n",
        "  X = data[:,coloms]\n",
        "  Y = data[:,6]\n",
        "  del data\n",
        "  # correct window to get constant time samples \n",
        "  #window2 = math.floor(window * 1/hz)\n",
        "  window2 = window\n",
        "  print(window2)\n",
        "  # coloms = the colloms for X \n",
        "  # window = the window size in samples \n",
        "  # epochs, lr are the epochs and leuring rate for training \n",
        "  data_len = math.floor(data_len * len(X))\n",
        "  #if data_len > len(data):\n",
        "  \n",
        "  print(\"length bevore subsampling = \", X.shape)\n",
        "  # super sample data (trow away every n the value)\n",
        "  if hz != 1:\n",
        "    sub = range(0,  len(X), hz)\n",
        "    X = np.delete(X,sub,0)\n",
        "  # set dt after subsampling\n",
        "  if 0 in coloms:\n",
        "    for i in range(len(X)-1):\n",
        "      X[i,0] = X[i,0] - X[i+1,0]\n",
        "    X = X[:-1,:]\n",
        "  print(\"length after = \", X.shape)\n",
        "  # limit X to max time window\n",
        "  sample_size = math.floor(len(X)/window2)\n",
        "  X3 = X[0:sample_size*window2,:]\n",
        "  # x1 = cut data into samples of lengt window time = +-window/100 sec\n",
        "  # Y1 = labels based on te first position of theplatform\n",
        "  X1 = np.zeros((sample_size, len(coloms), window2))\n",
        "  Y1 = np.zeros(sample_size)\n",
        "  for i in range(sample_size):\n",
        "    X1[i,:,:] = X3[i*window2:(i+1)*window2,:].T\n",
        "    Y1[i] = Y[i*window2]\n",
        "  # split data val and train \n",
        "  # 80 20 split\n",
        "  vel_lim = math.floor(sample_size*0.2)\n",
        "  X_train = X1[0:sample_size-vel_lim,:,:]\n",
        "  X_test = X1[sample_size-vel_lim:,:,:]\n",
        "\n",
        "  y_train = Y1[0:sample_size-vel_lim]\n",
        "  y_test = Y1[sample_size-vel_lim:]\n",
        "  \n",
        "  X, y, splits = combine_split_data([X_train, X_test], [y_train, y_test])\n",
        "  dsets = TSDatasets(X, y, splits=splits, inplace=True)\n",
        "  dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64, 256],batch_tfms=[TSStandardize()],num_workers=0,device=torch.device('cuda'))\n",
        "  dls.show_batch(sharey=True)\n",
        "  plt.show()\n",
        "  for i in range(len(coloms)):\n",
        "    plt.plot(X1[:,i,1])\n",
        "    plt.show()\n",
        "  plt.plot(Y1)\n",
        "\n",
        "\n",
        "  # create leurner \n",
        "  learn = TSRegressor(X, y, splits=splits, bs=[64, 256],batch_tfms=[TSStandardize(by_var=True)],device=torch.device('cuda'))\n",
        "  #model = InceptionTime(dls.vars, dls.c)\n",
        "\n",
        "  #learn = Learner(dls, model, metrics=accuracy,loss_func = torch.nn.L1Loss()) \n",
        "\n",
        "  if lr == \"auto\":\n",
        "    lrfind = learn.lr_find()\n",
        "    lr = lrfind.valley\n",
        "    print(lr)\n",
        "  # fit the model \n",
        "  learn.fit_one_cycle(epochs, lr, cbs=[fastai.callback.tracker.EarlyStoppingCallback(monitor='valid_loss', comp=None, min_delta=0.03,patience=15, reset_on_fit=True)])\n",
        "  learn.export('/Users/emile/Downloads/model_dump/model_{}_{}_{}_{}_{}_{}'.format(coloms, window, epochs, lr,hz,data_len))\n",
        "  learn.recorder.plot_metrics()\n",
        "  dls = learn.dls\n",
        "  valid_dl = dls.valid\n",
        "  valid_probas, valid_targets, valid_preds = learn.get_preds(dl=valid_dl, with_decoded=True)\n",
        "  g = valid_targets-valid_preds[:,0]\n",
        "  loss = torch.sqrt(torch.mean(g**2))\n",
        "  fig = plt.figure(figsize=(10,5))\n",
        "  plt.plot(valid_targets[:])\n",
        "  plt.plot(valid_preds[:])\n",
        "  # Labeling the X-axis \n",
        "  plt.xlabel('time(seconds)') \n",
        "  # Labeling the Y-axis \n",
        "  plt.ylabel('rotation') \n",
        "  plt.legend() \n",
        "  print(loss)\n",
        "  fig.savefig('/Users/emile/Downloads/image_dump/image_{}_{}_{}_{}_{}_{}.jpg'.format(coloms, window, epochs, lr,hz,data_len), bbox_inches='tight', dpi=300)\n",
        "  X\n",
        "  return learn, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MutbLSqGgqjG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuqDbejjYuXC"
      },
      "outputs": [],
      "source": [
        "accelero = [7,8,9]\n",
        "power = [3] # [3,4]\n",
        "vel = [5]\n",
        "time = [0]\n",
        "coloms = [3,4,7,8,9]\n",
        "epochs = 30\n",
        "lr = \"auto\"\n",
        "data_len = 2000000\n",
        "\n",
        "windows = [900,300,600,1200,100] # samples per prediction\n",
        "lengthes_data = [2000000] # procentile of the data used \n",
        "#frequenties = [20, 40, 60, 80, 100] # procentile of sample rate to be used \n",
        "frequenties = [1, 2, 3, 4, 5] # trow away one sample every x \n",
        "colomsets = [[0,3,5,7,8,9],[0,3,5],[0,3,7,8,9],[0,5,7,8,9]] # sets of colloms leefing one per set\n",
        "\n",
        "#default \n",
        "\n",
        "window = 900\n",
        "epochs = 50\n",
        "lr = \"auto\"\n",
        "data_len = 1500000\n",
        "hz = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkugKe-_qQpv"
      },
      "outputs": [],
      "source": [
        "for data_len in lengthes_data:\n",
        "  for window in windows:\n",
        "    for hz in frequenties:\n",
        "      for coloms in colomsets:\n",
        "        learn2, loss = train_get_metrics(coloms, window, epochs, lr,data_len,hz)\n",
        "        print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3PBi_lMVyFt"
      },
      "outputs": [],
      "source": [
        "learn2, loss = train_get_metrics([0,3,5],  900, 20, lr,1500000,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mgy8INSs2TKr"
      },
      "outputs": [],
      "source": [
        "losses2 = np.zeros([4,5,20])\n",
        "for k in range(20):\n",
        "  j = -1\n",
        "  for hz in frequenties:\n",
        "    j = j + 1\n",
        "    i = -1\n",
        "    for coloms in colomsets:\n",
        "      i = i + 1\n",
        "      learn2, loss = train_get_metrics(coloms, window, epochs, lr,data_len,hz)\n",
        "      losses2[i,j,k] = loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZFRed_0pRnr"
      },
      "outputs": [],
      "source": [
        "learn2.recorder.plot_metrics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UU-6k2Jgl0L1"
      },
      "outputs": [],
      "source": [
        "print(a.valley)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mt5lZ5SPbgWW"
      },
      "outputs": [],
      "source": [
        "#learn2.show_results()\n",
        "learn2.export()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Pq3YnkfcmNp"
      },
      "outputs": [],
      "source": [
        "dls = learn2.dls\n",
        "valid_dl = dls.valid\n",
        "valid_probas, valid_targets, valid_preds = learn2.get_preds(dl=valid_dl, with_decoded=True)\n",
        "g = valid_targets-valid_preds[:,0]\n",
        "plt.plot(valid_targets[:1000])\n",
        "plt.plot(valid_preds[:1000,0])\n",
        "plt.plot(g[:1000])\n",
        "plt.show()\n",
        "print(torch.mean(g**2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjGS71vDr1pk"
      },
      "outputs": [],
      "source": [
        "valid_preds.shape"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "3edaea292c0121a4edf57fb9b1fd8f07f262e12aa8a67f7e3de01891f262632f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
